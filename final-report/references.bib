@article{RN5,
   author = {Cutkosky, Ashok and Orabona, Francesco},
   title = {Momentum-Based Variance Reduction in Non-Convex SGD},
   journal = {arXiv pre-print server},
   DOI = {None
arxiv:1905.10018},
   url = {https://arxiv.org/abs/1905.10018},
   year = {2020},
   type = {Journal Article}
}

@article{RN3,
   author = {Lam and Liu, Jie and Scheinberg, Katya and Tak\'a\v, Martin},
   title = {SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient},
   journal = {arXiv pre-print server},
   note = {{c}},
   DOI = {None
arxiv:1703.00102},
   url = {https://arxiv.org/abs/1703.00102},
   year = {2017},
   type = {Journal Article}
}

@article{lecun,
   author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
   title = {Deep learning},
   journal = {Nature},
   volume = {521},
   number = {7553},
   pages = {436-444},
   abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
   ISSN = {1476-4687},
   DOI = {10.1038/nature14539},
   url = {https://doi.org/10.1038/nature14539},
   year = {2015},
   type = {Journal Article}
}

@article{pagepaper,
   author = {Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt\'arik, Peter},
   title = {PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization},
   journal = {arXiv pre-print server},
   DOI = {None
arxiv:2008.10898},
   url = {https://arxiv.org/abs/2008.10898},
   year = {2020},
   type = {Journal Article}
}

@misc{RN6,
   author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
   title = {Policy gradient methods for reinforcement learning with function approximation},
   publisher = {MIT Press},
   pages = {1057–1063},
   year = {1999},
   type = {Conference Paper}
}

@article{stormpaper,
   author = {Yuan, Huizhuo and Lian, Xiangru and Liu, Ji and Zhou, Yuren},
   title = {Stochastic Recursive Momentum for Policy Gradient Methods},
   journal = {arXiv pre-print server},
   DOI = {None
arxiv:2003.04302},
   url = {https://arxiv.org/abs/2003.04302},
   year = {2020},
   type = {Journal Article}
}

@article{gpomdp,
  author    = {Peter L. Bartlett and
               Jonathan Baxter},
  title     = {Infinite-Horizon Policy-Gradient Estimation},
  journal   = {CoRR},
  volume    = {abs/1106.0665},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.0665},
  eprinttype = {arXiv},
  eprint    = {1106.0665},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1106-0665.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{RN17,
  author    = {Tianbing Xu and
               Qiang Liu and
               Jian Peng},
  title     = {Stochastic Variance Reduction for Policy Gradient Estimation},
  journal   = {CoRR},
  volume    = {abs/1710.06034},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.06034},
  eprinttype = {arXiv},
  eprint    = {1710.06034},
  timestamp = {Tue, 04 Jun 2019 21:08:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-06034.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{RN4,
   author = {Yuan, Huizhuo and Lian, Xiangru and Liu, Ji and Zhou, Yuren},
   title = {Stochastic Recursive Momentum for Policy Gradient Methods},
   journal = {arXiv pre-print server},
   DOI = {None
arxiv:2003.04302},
   url = {https://arxiv.org/abs/2003.04302},
   year = {2020},
   type = {Journal Article}
}

@article{RN12,
  author    = {Matteo Papini and
               Damiano Binaghi and
               Giuseppe Canonaco and
               Matteo Pirotta and
               Marcello Restelli},
  title     = {Stochastic Variance-Reduced Policy Gradient},
  journal   = {CoRR},
  volume    = {abs/1806.05618},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.05618},
  eprinttype = {arXiv},
  eprint    = {1806.05618},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-05618.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{reinforcer,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{baxter2001infinite,
  title={Infinite-horizon policy-gradient estimation},
  author={Baxter, Jonathan and Bartlett, Peter L},
  journal={Journal of Artificial Intelligence Research},
  volume={15},
  pages={319--350},
  year={2001}
}

@inproceedings{papini2018stochastic,
  title={Stochastic variance-reduced policy gradient},
  author={Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
  booktitle={International conference on machine learning},
  pages={4026--4035},
  year={2018},
  organization={PMLR}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@misc{baker2020emergent,
      title={Emergent Tool Use From Multi-Agent Autocurricula}, 
      author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
      year={2020},
      eprint={1909.07528},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{williams,
   author = {Williams, Ronald J.},
   title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
   journal = {Machine Learning},
   volume = {8},
   number = {3},
   pages = {229-256},
   abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
   ISSN = {1573-0565},
   DOI = {10.1007/BF00992696},
   url = {https://doi.org/10.1007/BF00992696},
   year = {1992},
   type = {Journal Article}
}

@article{qlearning,
   author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
   title = {Human-level control through deep reinforcement learning},
   journal = {Nature},
   volume = {518},
   number = {7540},
   pages = {529-533},
   abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
   ISSN = {1476-4687},
   DOI = {10.1038/nature14236},
   url = {https://doi.org/10.1038/nature14236},
   year = {2015},
   type = {Journal Article}
}

@inproceedings{importancesampling,
  title={Eligibility Traces for Off-Policy Policy Evaluation},
  author={Doina Precup and Richard S. Sutton and Satinder Singh},
  booktitle={ICML},
  year={2000}
}


\documentclass[10pt,twocolumn,letterpaper]{article}


%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[bottom]{footmisc}
\renewcommand{\thefootnote}{\arabic{footnote}}

%% Sets page size and margins
\usepackage[a4paper,top=1 cm,bottom=2cm,left=1cm,right=1cm,marginparwidth=2cm, marginparsep=2cm]{geometry}
\usepackage{titlesec}




%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[
backend=biber,
style=ieee,
]{biblatex}
\addbibresource{references.bib}
\usepackage{sectsty}




%% Title
\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\begin{flushleft}
		    \vspace{-0.5em}

		    \large {Project proposal : Probabilistic Gradient Estimator for Policy Gradient Methods \vspace{0em}}\newline
    		\scriptsize {\normalfont
    		{Emiljo Mehillaj\footnote{Emiljo Mehillaj \url{emehillaj@student.ethz.ch} 20-952-420}}, 
    		{Vasilii Kopylov\footnote{Vasilii Kopylov \url{vkopylov@student.ethz.ch} - 20-943-825}}, 
    		{Jakub Mandula\footnote{Jakub Mandula \url{jmandula@student.ethz.ch} - 20-961-861}}, 
    		{Christian Gasse\footnote{Christian Gasse \url{chgasser@student.ethz.ch} - 15-830-516}}}
    
    		\vspace{-6em}
		\end{flushleft}

}
\sectionfont{\fontsize{12}{15}\selectfont}
\titlespacing*{\section}
{0pt}{5.5ex plus 1ex minus .2ex}{4.3ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{5.5ex plus 1ex minus .2ex}{4.3ex plus .2ex}
\date{}

\begin{document}

\maketitle
\selectlanguage{english}


Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces but suffer from a high variance of gradient estimates. Variance reduction techniques for solving non-convex problems have been recently explored for policy gradient-based problems \cite{RN1,papini2018stochastic}. They showed an advantage over traditional policy gradient estimators (e.g., REINFORCE\cite{reinforcer} and GPOMDP\cite{RN14}) both in sample complexity and training stability. However, most of these methods have a large number of tunable hyperparameters. In this work, we introduce a novel algorithm - Probabilistic Gradient Estimator for Policy Gradient (PAGE-PG) - which has fewer hyperparameters and is expected to reach a superior convergence rate. 

\section{Introduction \& Literature Review}
Policy gradient (PG) methods learn the policy parameter based on the gradient of some scalar performance measure $J(\theta)$ with respect to the policy parameters \cite{sutton2018reinforcement}. One way of expressing $J(\theta)$ is as the expected total reward given below: 
\begin{equation} \label{eq: 1}
\begin{aligned}
J(\theta)
&=\mathbb{E}_{\tau \sim \pi_{\theta}} r(\tau)
\end{aligned}
\end{equation}
where $ \tau $ is a trajectory that contains states and actions involved in an episodic task.
Usually, this problem is solved by gradient descent (GD) where the gradients are calculated from Equation (\ref{eq: 1}) as:
\begin{equation} \label{eq: 2}
\nabla J(\theta)=\nabla \mathbb{E}_{\tau \sim \pi_{\theta}} r(\tau)=\mathbb{E}_{\tau \sim \pi_{\theta}}\left[r(\tau) \nabla \log \pi_{\theta}(\tau)\right]
\end{equation}

Computation of this gradient is costly and we need to estimate it. A straightforward  estimator is the Monte Carlo estimator shown below:

\begin{equation} \label{eq: 3}
\hat{\nabla} J(\theta)=\frac{1}{N} \sum_{i=0}^{N}\left[r(\tau) \nabla \log \pi_{\theta}(\tau)\right]
\end{equation}

However, two common problems that arise during training via PG are the distribution shift and the high inherent training variance  \cite{RN12}. Therefore, several variance-reduced policy gradient methods have been proposed to address these problems. Some examples of such methods include:
\begin{enumerate}
    \item \textbf{GPOMDP:} This method reduces the variance of the gradient estimator in Equation (\ref{eq: 3}) through the subtraction of a state-dependent baseline \cite{RN14}. We will refer to $d_{i,unb}(\theta)$ as the GPOMDP unbiased estimator per trajectory $i$ that is in turn forward sampled according to the policy $\pi_{\theta}$. The total GPOMDP estimator will be an average of all trajectory estimators.  

    \item \textbf{SVRG:} This estimator addresses the distribution shift issue in RL through the introduction of importance sampling between trajectories generated by different policy parametrizations \cite{RN17}.
    \item \textbf{SARAH:} This method is similar to SVRG but is more memory efficient by waiving the requirement of storing past gradients \cite{RN3}. 
    \item \textbf{STORM-PG:} This gradient estimator combines the unbiased estimator introduced by GPOMDP and SARAH. A tunable hyperparameter controls the relative weights of these two estimators. This estimator showed improved gradient complexity in RL setting \cite{RN1}.
\end{enumerate}
Recently, a novel stochastic gradient estimator has been introduced for non-convex approximation: Probabilistic Gradient Estimator (PAGE). This method relies on probabilistic switching between the vanilla SGD and SARAH and shows superior convergence results in models trained on MNIST, LeNet, etc. \cite{RN2}.

\section{Methods}

In this project, we will adapt PAGE for policy gradients in RL. To the best of our knowledge, a PAGE adjustment to the RL setting has not yet been implemented. Our motivation for the PAGE-PG lies in the favorable convergence results of PAGE in online settings. PAGE exhibits convergence rates that are on par with SARAH and STORM when the PL condition is not satisfied. When such a condition is met locally, the convergence rate becomes linear \cite{RN2}.  Another reason why PAGE is attractive for the RL setting is that we can fix the hyperparameter $p_{t}$ to an empirically suitable value as done in the original PAGE paper. This is another advantage over most other variance-reduced methods which have a lot of trainable hyperparameters.  

In this project, we will first implement the vanilla SGD part of our PAGE-PG using the Monte Carlo estimator shown in Equation (\ref{eq: 3}). Then, drawing inspiration from STORM-PG, we will implement another version of PAGE-PG using the GPOMDP unbiased estimator. In this case, the PAGE-PG gradient estimate will look as shown below:
\begin{equation} \label{eq: 4}
\mathbf{g}_{t+1}= \begin{cases}\frac{1}{b} \sum_{i \in I} d_{i,unb}\right) &\\ \text { with probability } p_{t} \\ \\ \mathbf{g}_{t+1}+\frac{1}{b^{\prime}} \sum_{i \in I^{\prime}}\left(d_{i,unb}\left({\theta}_{t+1}\right)-d_{i,unb}^{{\theta}_{t+1}}({\theta}_{t})\right) &\\ \text { with probability } 1-p_{t}\end{cases}
\end{equation}

Furthermore, we plan on exploring better baselines other than the one used in GPOMDP to test the performance of our suggested algorithm. 
\section{Experiments}

We will conduct a set of experiments to validate our expectations regarding PAGE-PG and benchmark its performance. We will compare it with the baseline algorithms such as SARAH, SVRG, GPOMDP, STORM-PG in the Cart-Pole and Mountain-Car tasks to evaluate both its computational complexity and convergence. And if time allows, we would also benchmark on larger-scale problems using the OpenAI Gym \cite{brockman2016openai} or the Multi-Agent Emergence Environments \cite{baker2020emergent}. 


\printbibliography

\end{document}